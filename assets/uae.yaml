---
- type: model
  name: Falcon-40B
  organization: UAE Technology Innovation Institute
  description: Falcon-40B is a 40B parameters causal decoder-only model built by
    TII and trained on 1,000B tokens ofÂ RefinedWeb enhanced with curated corpora.
  created_date: 2023-06-14
  url: https://huggingface.co/tiiuae/falcon-40b
  model_card: https://huggingface.co/tiiuae/falcon-40b
  modality: text; text
  analysis: ''
  size: 40B parameters (dense)
  dependencies: [RefinedWeb]
  training_emissions: ''
  training_time: 2 months
  training_hardware: 384 A100 40GB GPUs
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: irresponsible or harmful use or production use without adequate
    assessment of risks and mitigation
  monitoring: None
  feedback: https://huggingface.co/tiiuae/falcon-40b/discussions
- type: dataset
  name: RefinedWeb
  organization: UAE Technology Innovation Institute
  description: RefinedWeb is a high-quality five trillion tokens web-only English
    pretraining dataset.
  created_date: 2023-06-01
  url: https://arxiv.org/pdf/2306.01116.pdf
  datasheet: https://huggingface.co/datasets/tiiuae/falcon-refinedweb
  modality: text
  size: 600B tokens
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: open
  license:
    explanation: License can be found at https://huggingface.co/datasets/tiiuae/falcon-refinedweb
    value: custom
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Falcon-180B
  organization: UAE Technology Innovation Institute
  description: Falcon-180B is a 180B parameters causal decoder-only model built
    by TII and trained on 3,500B tokens of RefinedWeb enhanced with curated corpora.
  created_date: 2023-09-06
  url: https://falconllm.tii.ae/falcon-models.html
  model_card: https://huggingface.co/tiiuae/falcon-180B
  modality: text; text
  analysis: Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT on the Open
    LLM Leaderboard at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.
  size: 180B parameters (dense)
  dependencies: [RefinedWeb]
  training_emissions: ''
  training_time: 9 months
  training_hardware: 4096 A100 40GB GPUs
  quality_control: ''
  access: open
  license: unknown
  intended_uses: ''
  prohibited_uses: Production use without adequate assessment of risks and mitigation;
    any use cases which may be considered irresponsible or harmful.
  monitoring: None
  feedback: https://huggingface.co/tiiuae/falcon-180b/discussions
