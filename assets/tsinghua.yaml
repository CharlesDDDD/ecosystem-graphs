---
- access:
    explanation: Model weights are available but gated via an [[application form]](https://models.aminer.cn/codegeex/download/request)
    value: limited
  analysis: none
  created_date: 2022-09-20
  dependencies: []
  description: CodeGeeX is an autoregressive language model trained on code
  feedback: none
  intended_uses: none
  license:
    explanation: The license is provided in the [[Github repository]](https://github.com/THUDM/CodeGeeX)
    value: Apache 2.0
  modality:
    explanation: code
    value: code; code
  model_card: none
  monitoring: none
  name: CodeGeeX
  organization: Tsinghua
  prohibited_uses: none
  quality_control: none
  size: 13B parameters (dense)
  training_emissions: unknown
  training_hardware: THUDM 1536 Ascend 910 (32GB) Cluster
  training_time: unknown
  type: model
  url: https://github.com/THUDM/CodeGeeX
- access:
    explanation: Model checkpoints available from [[Wudao-Wenhui]](https://resource.wudaoai.cn/home?ind=2&name=WuDao%20WenHui&id=1399364355975327744)
    value: open
  analysis: ''
  created_date:
    explanation: The date the model paper was released
    value: 2021-05-26
  dependencies: []
  description: CogView is a transformer model for text-to-image generation
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The license is provided in the [[Github repository]](https://github.com/THUDM/CogView)\n"
    value: Apache 2.0
  modality: {}
  model_card: none
  monitoring: ''
  name: CogView
  organization: Tsinghua
  prohibited_uses: ''
  quality_control: ''
  size: ''
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2105.13290
- access:
    explanation: The model checkpoints are available for download from [[BAAI]](https://model.baai.ac.cn/model-detail/100041)
    value: open
  analysis: ''
  created_date:
    explanation: The date the model paper was released
    value: 2022-04-28
  dependencies: []
  description: CogView 2 is a hierarchical transformer for text-to-image generation
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The license is provided in the [[Github repository]](https://github.com/THUDM/CogView2)\n"
    value: Apache 2.0
  modality: {}
  model_card: none
  monitoring: ''
  name: CogView 2
  organization: Tsinghua
  prohibited_uses: ''
  quality_control: ''
  size: ''
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2204.14217
- access:
    explanation: Model checkpoints are available for download from https://github.com/THUDM/CogVideo
    value: open
  analysis: ''
  created_date:
    explanation: The date the model paper was released
    value: 2022-05-29
  dependencies: []
  description: CogVideo is a transformer model for text-to-video generation
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The license is provided in the [[Github repository]](https://github.com/THUDM/CogVideo)\n"
    value: Apache 2.0
  modality: {}
  model_card: none
  monitoring: ''
  name: CogVideo
  organization: Tsinghua
  prohibited_uses: ''
  quality_control: ''
  size: ''
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2205.15868
- access:
    explanation: Model checkpoints are available from the [[GitHub repository]](https://github.com/THUDM/GLM-130B/blob/main/MODEL_LICENSE)
    value: open
  analysis: ''
  created_date:
    explanation: The date the model website was made public
    value: 2022-08-04
  dependencies:
    - The Pile
    - GLM-130B Chinese corpora
    - P3
    - DeepStruct finetuning dataset
  description: GLM-130B is a bidirectional language model trained on English and
    Chinese
  feedback: ''
  intended_uses: ''
  license:
    explanation: Unique model license. See the [[GitHub repository]](https://github.com/THUDM/GLM-130B/blob/main/MODEL_LICENSE)
    value: GLM-130B License
  modality:
    explanation: text (Chinese | English)
    value: text; text
  model_card: none
  monitoring: ''
  name: GLM-130B
  organization: Tsinghua
  prohibited_uses: ''
  quality_control: ''
  size: 130B parameters (dense)
  training_emissions: ''
  training_hardware: THUDM 96 DGX-A100 (40G) cluster
  training_time: ''
  type: model
  url: https://keg.cs.tsinghua.edu.cn/glm-130b/
