---
- access: open
  analysis: ''
  created_date: 2022-11-29
  dependencies: [GPT-J, P3, NaturalInstructions-v2]
  description: ''
  feedback: ''
  intended_uses: ''
  license: Apache 2.0
  modality:
    explanation: text
    value: text; text
  model_card: ''
  monitoring: ''
  name: GPT-JT
  organization: Together
  prohibited_uses: ''
  quality_control: ''
  size: 6B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai
- access: open
  analysis: ''
  created_date: 2023-03-10
  dependencies: [GPT-NeoX, OIG-43M]
  description: ''
  feedback: ''
  intended_uses: ''
  license: Apache 2.0
  modality:
    explanation: text
    value: text; text
  model_card: ''
  monitoring: ''
  name: GPT-NeoXT-Chat-Base
  organization: Together
  prohibited_uses: ''
  quality_control: ''
  size: 20B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://www.together.xyz/blog/openchatkit
- access: open
  analysis: ''
  created_date: 2023-03-10
  dependencies: [GPT-JT, OIG-moderation]
  description: ''
  feedback: ''
  intended_uses: ''
  license: Apache 2.0
  modality:
    explanation: text
    value: text; text
  model_card: ''
  monitoring: ''
  name: OpenChatKit moderation model
  organization: Together
  prohibited_uses: ''
  quality_control: ''
  size: 6B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://www.together.xyz/blog/openchatkit
- access: open
  analysis: ''
  created_date: 2023-03-10
  datasheet: ''
  dependencies: [P3, NaturalInstructions-v2, FLAN dataset]
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: Apache 2.0
  modality: text
  monitoring: ''
  name: OIG-43M
  organization: Together, LAION, Ontocord
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 43M instructions
  type: dataset
  url: https://laion.ai/blog/oig-dataset/
- access: open
  analysis: ''
  created_date: 2023-03-10
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: Apache 2.0
  modality: text
  monitoring: ''
  name: OIG-moderation
  organization: Together, LAION, Ontocord
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: unknown
  type: dataset
  url: https://laion.ai/blog/oig-dataset/
- access: open
  analysis: ''
  created_date: 2022-04-17
  datasheet: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T
  dependencies: [GitHub, Wikipedia]
  description: The RedPajama base dataset is a 1.2 trillion token fully-open dataset
    created by following the recipe described in the LLaMA paper
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: Apache 2.0
  modality: {}
  monitoring: ''
  name: RedPajama-Data
  organization: Together
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 1.2 trillion tokens
  type: dataset
  url: https://www.together.xyz/blog/redpajama
- access: open
  analysis: Model evaluated over AlpacaEval, Rouge score over BookSum, and accuracy over MQA.
  created_date: 2023-08-18
  dependencies: [LLaMA-2-70B-Chat outputs, BookSum dataset, MQA dataset, Together API, LLaMA-2-7B-32K]
  description: Llama-2-7B-32K-Instruct is an open-source, long-context chat model finetuned from Llama-2-7B-32K, over high-quality instruction and chat data.
  feedback: https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct/discussions
  intended_uses: ''
  license: Apache 2.0
  modality:
    explanation: text
    value: text; text
  model_card: https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct
  monitoring: ''
  name: Llama-2-7B-32K-Instruct
  organization: Together
  prohibited_uses: ''
  quality_control: ''
  size: 7B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://together.ai/blog/llama-2-7b-32k-instruct

