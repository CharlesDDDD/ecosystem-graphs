---
- access:
    explanation: "The dataset access is limited to DeepMind researchers [[Model\
      \ Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.C).\n"
    value: closed
  analysis: "MassiveText data was analyzed for toxicity, language distribution,\
    \ URL breakdown, and tokenizer compression rates on the subsets [[Section A.2]](https://arxiv.org/pdf/2112.11446.pdf#subsection.A.2).\n"
  created_date:
    explanation: "The date that Gopher was announced [[DeepMind Blog Post]] (https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval).\n"
    value: 2021-12-08
  datasheet: https://arxiv.org/pdf/2112.11446.pdf#subsection.A.5
  dependencies: []
  description: "The MassiveText dataset was used to train the Gopher model.\n"
  excluded: "Documents that are not in English are excluded.\n"
  feedback:
    explanation: "The internal feedback mechanisms for WebText are unknown [[Model\
      \ Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.C).\n"
    value: unknown
  included: "MassiveText data come from 6 sources: MassiveWeb (48%), Books (27%),\
    \ C4 (10%), News (10%), GitHub (3%), and Wikipedia (2%). MassiveWeb is a web\
    \ text corpus curated for MassiveText.\n"
  intended_uses: "Pre-training of language models by DeepMind researchers [[Model\
    \ Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.C).\n"
  license:
    explanation: "The model likely has a license specifically for DeepMind's use,\
      \ based on the information provided in the datasheet [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#subsection.A.5).\n"
    value: unknown
  modality: {}
  monitoring:
    explanation: "There is no information on how DeepMind is internally monitoring\
      \ the use of the dataset.\n"
    value: unknown
  name: MassiveText
  organization: DeepMind
  prohibited_uses:
    explanation: "There are no known prohibited uses of the dataset, but the authors\
      \ state that it should not be used for training models with multilingual capabilities\
      \ as it only contains the English language [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.C).\n"
    value: unknown
  quality_control: "The authors use simple heuristics for filtering low quality\
    \ documents as opposed to relying on a classifier based on a \"gold\" set such\
    \ as the English Wikipedia, which could \"inadvertently bias towards a certain\
    \ demographic or erase certain dialects or sociolects from representation.\"\
    \ MassiveWeb subset was filtered using Googleâ€™s SafeSearch filter, preferring\
    \ it over to word filters that \"disproportinately filter out inoffensive content\
    \ associated with minority groups. MassiveWeb was filtered further for word\
    \ or phrase repetitions. All the subsets were filtered for document deduplication\
    \ and test set contamination\" [[Appendix A]](https://arxiv.org/pdf/2112.11446.pdf#appendix.A).\n"
  sample: []
  size: 10.5 TB
  type: dataset
  url: https://arxiv.org/pdf/2112.11446.pdf
- access:
    explanation: "The dataset access is limited to DeepMind researchers [[Datasheet]](https://arxiv.org/pdf/2204.14198.pdf#appendix.F).\n"
    value: closed
  analysis: ''
  created_date:
    explanation: "The date that Flamingo was posted to arXiv [[arXiv]] (https://arxiv.org/pdf/2204.14198.pdf).\n"
    value: 2022-04-29
  datasheet: https://arxiv.org/pdf/2204.14198.pdf#appendix.F
  dependencies: []
  description: "M3W (MassiveWeb) is dataset used to train Flamingo, and other vision-language\
    \ models and was created by researchers and engineers.\n"
  excluded: unknown
  feedback:
    explanation: "No feedback mechanism is mentioned in the datasheet [[Datasheet]](https://arxiv.org/pdf/2204.14198.pdf#appendix.F).\n"
    value: none
  included: "M3W has interleaved images (185M) and text (182GB) from the web.\n"
  intended_uses: "Pre-training of vision and language models by DeepMind researchers\
    \ [[Datasheet]](https://arxiv.org/pdf/2204.14198.pdf#appendix.F).\n"
  license:
    explanation: "The model likely has a license specifically for DeepMind's use,\
      \ based on the information provided in the datasheet [[Datasheet]](https://arxiv.org/pdf/2204.14198.pdf#appendix.F).\n"
    value: unknown
  modality: {}
  monitoring:
    explanation: "There is no information on how DeepMind is internally monitoring\
      \ the use of the dataset.\n"
    value: unknown
  name: M3W
  organization: DeepMind
  prohibited_uses:
    explanation: "There are no known prohibited uses of the dataset [[Datasheet]](https://arxiv.org/pdf/2204.14198.pdf#appendix.F).\n"
    value: unknown
  quality_control: "The authors provide a basic description of data processing and\
    \ cleaning.\n"
  sample: []
  size: 182GB Text, 185M Images
  type: dataset
  url: https://arxiv.org/pdf/2204.14198.pdf
- access:
    explanation: "The full dataset is not directly provided by the authors, though\
      \ some underlying data is public whereas others (e.g. MassiveText) is not.\n"
    value: closed
  analysis: "The Gato dataset compiles many datasets introduced in prior works,\
    \ with associated analyses.\n"
  created_date:
    explanation: "The date that Gato was announced [[DeepMind Blog Post]] (https://www.deepmind.com/blog/a-generalist-agent).\n"
    value: 2022-05-12
  datasheet: none
  dependencies: [MassiveText]
  description: "The Gato datasets are a collection of data used to train the Gato\
    \ model.\n"
  excluded:
    explanation: "No specific filtering is mentioned in the Gato paper.\n"
    value: none
  feedback:
    explanation: "There is no mention on feedback mechanisms either internally or\
      \ externally.\n"
    value: none
  included: "The full composition of the dataset across individual sources can be\
    \ found in the paper.\n"
  intended_uses:
    explanation: "There are no known intended uses of the dataset stated by authors\
      \ beyond training Gato.\n"
    value: unknown
  license:
    explanation: "The datasets have individual licenses, but no overall license\
      \ is mentioned by the authors.\n"
    value: unknown
  modality: {}
  monitoring:
    explanation: "There is no mention on how DeepMind is internally monitoring the\
      \ use of the dataset.\n"
    value: none
  name: Gato dataset
  organization: DeepMind
  prohibited_uses:
    explanation: "There are no known prohibited uses of the dataset stated by authors.\n"
    value: unknown
  quality_control: unknown
  sample: []
  size: 10.5 TB Text, 2.2B Text-Image pairs, 1.5T tokens of simulated control, 500k
    robotics trajectories
  type: dataset
  url: https://www.deepmind.com/blog/a-generalist-agent
- access:
    explanation: Models are available for download from the [[Github repository]](https://github.com/deepmind/alphafold)
    value: open
  analysis: ''
  created_date:
    explanation: The date the model paper was released
    value: 2021-07-15
  dependencies: [Protein Data Bank]
  description: AlphaFold2 is a protein language model trained on protein sequences
  feedback: ''
  intended_uses: ''
  license:
    explanation: The license is provided in the [[Github repository]](https://github.com/deepmind/alphafold)
    value: Apache 2.0
  modality: {}
  model_card: none
  monitoring: ''
  name: AlphaFold2
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  size: ''
  training_emissions: ''
  training_hardware:
    explanation: Described in paper
    value: 128 TPUv3 cores
  training_time:
    explanation: Training takes "approximately 1 week" and finetuning takes "approximately
      4 days"
    value: 11 days
  type: model
  url: https://www.nature.com/articles/s41586-021-03819-2
- access:
    explanation: "The model has not been released and no discussion of release is\
      \ stated in the model card [[Model Card]](https://arxiv.org/pdf/2204.14198.pdf#appendix.E).\n"
    value: closed
  analysis: "Model performance was evaluated on image and video datasets primarily,\
    \ including dialogue.\n"
  created_date:
    explanation: "The date that Flamingo was posted to arXiv [[arXiv]] (https://arxiv.org/abs/2204.14198).\n"
    value: 2022-04-29
  dependencies: [M3W, ALIGN, LTIP, VTP, Chinchilla]
  description: "Flamingo is a Visual Language Model using the Transformer architecture\
    \ that is intended for few-shot learning.\n"
  feedback:
    explanation: "No contact information is provided for feedback in the model card\
      \ [[Model Card]](https://arxiv.org/pdf/2204.14198.pdf#appendix.E).\n"
    value: none
  intended_uses: "The intended uses are stated in the model card: \"The primary\
    \ use is research on visual language models (VLM), including: research on VLM\
    \ applications like classification, captioning or visual question answering,\
    \ understanding how strong VLMs can contribute to AGI, advancing fairness and\
    \ safety research in the area of multimodal research, and understanding limitations\
    \ of current large VLMs.\" [[Model Card]](https://arxiv.org/pdf/2204.14198.pdf#appendix.E).\n"
  license:
    explanation: "No license is provided in the model card [[Model Card]](https://arxiv.org/pdf/2204.14198.pdf#appendix.E).\n"
    value: unknown
  modality: {}
  model_card: https://arxiv.org/pdf/2204.14198.pdf#appendix.E
  monitoring:
    explanation: "There is no information on how DeepMind is internally monitoring\
      \ the use of the model.\n"
    value: unknown
  name: Flamingo
  organization: DeepMind
  prohibited_uses: "The model card lists the following as out of scope uses of the\
    \ model: \"Uses of the model for visually conditioned language generation in\
    \ harmful or deceitful settings. Broadly speaking, the model should not be used\
    \ for downstream applications without further safety and fairness mitigations\
    \ specific to each application.\" [[Model Card]](https://arxiv.org/pdf/2204.14198.pdf#appendix.E).\n"
  quality_control:
    explanation: "Reported in the mitigations in the model card [[Model Card]](https://openreview.net/pdf?id=1ikK0kHjvj#appendix.A).\n"
    value: none
  size: ''
  training_emissions:
    explanation: "Authors do not report the training emissions.\n"
    value: unknown
  training_hardware:
    explanation: "Reported in the paper checklist [[Checklist]](https://arxiv.org/pdf/2204.14198.pdf).\n"
    value: TPU
  training_time:
    explanation: "Reported in the paper checklist [[Checklist]](https://arxiv.org/pdf/2204.14198.pdf).\n"
    value: 15 days on 1536 TPUs
  type: model
  url: https://arxiv.org/pdf/2204.14198.pdf
- access:
    explanation: DeepMind does not provide access to AlphaCode to external researchers
    value: closed
  analysis: ''
  created_date:
    explanation: The date the model paper was released
    value: 2022-02-02
  dependencies: []
  description: AlphaCode is an autoregressive language model trained on code
  feedback: ''
  intended_uses: ''
  license:
    explanation: ''
    value: unknown
  modality: {}
  model_card: ''
  monitoring: ''
  name: AlphaCode
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  size: ''
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2203.07814
- access:
    explanation: "The model access is limited to DeepMind researchers. The model\
      \ won't be released to the public [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.B).\n"
    value: closed
  analysis: "Model performance was evaluated and analyzed on 152 NLP tasks including:\
    \ Language Modelling (20), Reading Comprehension (3), Fact Checking (3), Question\
    \ Answering (3), Common Sense (4), MMLU (57), BIG-bench (62) [[Section 4]](https://arxiv.org/pdf/2112.11446.pdf#section.4);\
    \ on toxicity and bias datasets [[Section 5]](https://arxiv.org/pdf/2112.11446.pdf#section.5);\
    \ and on dialogue tasks [[Section 6]](https://arxiv.org/pdf/2112.11446.pdf#section.6).\n"
  created_date:
    explanation: "The date that Gopher was announced [[DeepMind Blog Post]] (https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval).\n"
    value: 2021-12-08
  dependencies: [MassiveText]
  description: "Gopher is an autoregressive language model based on the Transformer\
    \ architecture with two modifications: using RMSNorm instead of LayerNorm and\
    \ using relative positional encoding scheme instead of absolute positional encodings\
    \ [[Section 3]](https://arxiv.org/pdf/2112.11446.pdf#subsection.3.1).\n"
  feedback: "The feedback for the model can be provided at the email linked in the\
    \ model card, geoffreyi at google.com [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.B).\n"
  intended_uses: "The intended uses are stated in the Gopher model card: \"The primary\
    \ use is research on language models, including: research on NLP applications\
    \ like machine translation and question answering, understanding how strong\
    \ language models can contribute to AGI, advancing fairness and safety research,\
    \ and understanding limitations of current LLMs\" [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.B).\n"
  license:
    explanation: "The model likely has a license specifically for DeepMind's use,\
      \ based on the information provided in the model card [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.B).\n"
    value: unknown
  modality: {}
  model_card: https://arxiv.org/pdf/2112.11446.pdf#appendix.B
  monitoring:
    explanation: "There is no information on how DeepMind is internally monitoring\
      \ the use of the model.\n"
    value: unknown
  name: Gopher
  organization: DeepMind
  prohibited_uses: "The model card lists the following as out of scope uses of the\
    \ model: \"for language generation in harmful or deceitful settings. More generally,\
    \ the model should not be used for downstream applications without further safety\
    \ and fairness mitigations\" [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.B).\n"
  quality_control: none
  size: ''
  training_emissions:
    explanation: "The training emission estimate from the paper [[Section F]](https://arxiv.org/pdf/2112.11446.pdf#appendix.F)\n"
    value: 380 tCO2e
  training_hardware:
    explanation: "Reported in the paper [[Section F]](https://arxiv.org/pdf/2112.11446.pdf#appendix.F).\n"
    value: TPUv3 pods
  training_time:
    explanation: "The authors reported the training petaflops for all of the 4 different\
      \ sizes of the model. For the 280B parameter model, the petaflops was reported\
      \ as 6.31E+08. We compute the Gopher's training time in petaflop/s-day as\
      \ 6.31E+08 / (60*60*24) = 7303.24 petaflop/s-day.\n"
    value: 7303.24 petaflop/s-day
  type: model
  url: https://arxiv.org/pdf/2112.11446.pdf
- access:
    explanation: "The model access is limited to DeepMind researchers. The model\
      \ won't be released to the public [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.B).\n"
    value: closed
  analysis: "Model performance was evaluated and analyzed on many NLP tasks including\
    \ language modeling, reading comprehension, question answering, commonsense-intensive\
    \ tasks, and the BIG-Bench and MMLU meta-benchmarks.\n"
  created_date:
    explanation: "The date that Chinchilla was posted on arXiv [[arXiv]] (https://arxiv.org/abs/2203.15556).\n"
    value: 2022-03-29
  dependencies: [MassiveText]
  description: "Chinchilla is an autoregressive language model based on the Transformer\
    \ architecture with improved scaling laws.\n"
  feedback: "The feedback for the model can be provided at the email linked in the\
    \ model card, {jordanhoffmann, sborgeaud, amensch,sifre} at deepmind.com [[Model\
    \ Card]](https://arxiv.org/pdf/2203.15556.pdf#appendix.I).\n"
  intended_uses: "The intended uses are stated in the Chinchilla model card: \"\
    The primary use is research on language models, including: research on the scaling\
    \ behaviour of language models along with those listed in Gopher paper\" [[Model\
    \ Card]](https://arxiv.org/pdf/2203.15556.pdf#appendix.I).\n"
  license:
    explanation: "The model likely has a license specifically for DeepMind's use,\
      \ based on the information provided in the model card [[Model Card]](https://arxiv.org/pdf/2203.15556.pdf#appendix.I).\n"
    value: unknown
  modality: {}
  model_card: https://arxiv.org/pdf/2203.15556.pdf
  monitoring:
    explanation: "There is no information on how DeepMind is internally monitoring\
      \ the use of the model.\n"
    value: Unknown
  name: Chinchilla
  organization: DeepMind
  prohibited_uses: "The model card lists the following as out of scope uses of the\
    \ model: \"for language generation in harmful or deceitful settings. More generally,\
    \ the model should not be used for downstream applications without further safety\
    \ and fairness mitigations\" [[Model Card]](https://arxiv.org/pdf/2203.15556.pdf#appendix.I).\n"
  quality_control: none
  size: ''
  training_emissions:
    explanation: "Authors do not report the training emissions.\n"
    value: Unknown
  training_hardware:
    explanation: "Reported in the paper [[Section 4]](https://arxiv.org/pdf/2203.15556.pdf).\n"
    value: TPUv3/TPUv4 pods
  training_time:
    explanation: "The authors reported the training petaflops for all models, including\
      \ hypothetical larger models. For the 70B parameter model, the petaflops was\
      \ reported as 5.76E+08. We compute the Gopher's training time in petaflop/s-day\
      \ as 5.76E+08 / (60*60*24) = 6666.66 petaflop/s-day.\n"
    value: 7303.24 petaflop/s-day
  type: model
  url: https://arxiv.org/pdf/2203.15556.pdf
- access:
    explanation: "The model access is limited to DeepMind researchers. The model\
      \ won't be released to the public [[Model Card]](https://openreview.net/pdf?id=1ikK0kHjvj#appendix.A).\n"
    value: closed
  analysis: "Model performance was evaluated on simulated and robotics task primarily,\
    \ including out-of-distribution and skill generalization.\n"
  created_date:
    explanation: "The date that Gato was announced [[DeepMind Blog Post]] (https://www.deepmind.com/blog/a-generalist-agent).\n"
    value: 2022-05-12
  dependencies: [Gato dataset]
  description: "Gato is a generalist agent based on sequence modeling using the\
    \ Transformer architecture to implement multi-modal, multi-task, multi-embodiment\
    \ generalist policy.\n"
  feedback: "The feedback for the model can be provided at the email linked in the\
    \ model card, reedscot at google.com [[Model Card]](https://openreview.net/pdf?id=1ikK0kHjvj#appendix.A).\n"
  intended_uses: "The intended uses are stated in the Gopher model card: \"Learn\
    \ to accomplish a wide variety of tasks from expert demonstrations, such as\
    \ playing video games, controlling simulated embodiments, and real world block\
    \ stacking.\" [[Model Card]](https://openreview.net/pdf?id=1ikK0kHjvj#appendix.A).\n"
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality: {}
  model_card: https://openreview.net/pdf?id=1ikK0kHjvj#appendix.B
  monitoring:
    explanation: "There is no information on how DeepMind is internally monitoring\
      \ the use of the model.\n"
    value: unknown
  name: Gato
  organization: DeepMind
  prohibited_uses: "The model card lists the following as out of scope uses of the\
    \ model: \"Not intended for commercial or production use. Military uses are\
    \ strictly prohibited.\" [[Model Card]](https://openreview.net/pdf?id=1ikK0kHjvj#appendix.A).\n"
  quality_control:
    explanation: "Reported in the mitigations in the model card [[Model Card]](https://openreview.net/pdf?id=1ikK0kHjvj#appendix.A).\n"
    value: none
  size: ''
  training_emissions:
    explanation: "Authors do not report the training emissions.\n"
    value: unknown
  training_hardware:
    explanation: "Reported in the paper [[Section 2.3]](https://openreview.net/pdf?id=1ikK0kHjvj).\n"
    value: 16x16 TPU v3 slice
  training_time:
    explanation: "Reported in the paper [[Section 2.3]](https://openreview.net/pdf?id=1ikK0kHjvj).\n"
    value: 4 days on a 16x16 TPU v3 slice
  type: model
  url: https://www.deepmind.com/blog/a-generalist-agent
- access: closed
  analysis: ''
  created_date: 2022-09-28
  dependencies:
    - Chinchilla
    - Google Search
    - Sparrow Rule reward model
    - Sparrow Preference reward model
  description: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality:
    explanation: text
    value: text; text
  model_card: ''
  monitoring: ''
  name: Sparrow
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  size: 70B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2209.14375
- access: closed
  analysis: ''
  created_date: 2021-12-08
  dependencies: [MassiveText]
  description: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality: {}
  model_card: ''
  monitoring: ''
  name: RETRO
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  size: ''
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2112.04426
- access: closed
  analysis: ''
  created_date: 2022-09-28
  dependencies: [Chinchilla, Sparrow adversarial probing dataset]
  description: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality:
    explanation: text
    value: text; text
  model_card: ''
  monitoring: ''
  name: Sparrow Rule reward model
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  size: 70B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2209.14375
- access: closed
  analysis: ''
  created_date: 2022-09-28
  dependencies: [Chinchilla, Sparrow response preference dataset]
  description: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality:
    explanation: text
    value: text; text
  model_card: ''
  monitoring: ''
  name: Sparrow Preference reward model
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  size: 70B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2209.14375
- access: closed
  analysis: ''
  created_date: 2022-09-28
  datasheet: ''
  dependencies: [Chinchilla]
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality: text
  monitoring: ''
  name: Sparrow adversarial probing dataset
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 27k ratings
  type: dataset
  url: https://arxiv.org/abs/2209.14375
- access: closed
  analysis: ''
  created_date: 2022-09-28
  datasheet: ''
  dependencies: [Chinchilla]
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality: text
  monitoring: ''
  name: Sparrow response preference dataset
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 72k comparisons
  type: dataset
  url: https://arxiv.org/abs/2209.14375
- access: closed
  analysis: ''
  created_date: 2022-03-16
  dependencies: [Gopher, Google Search, GopherCite reward model]
  description: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality: {}
  model_card: ''
  monitoring: ''
  name: GopherCite
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  size: ''
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://storage.googleapis.com/deepmind-media/Teaching%20language%20models%20to%20support%20answers%20with%20verified%20quotes/Teaching%20language%20models%20to%20support%20answers%20with%20verified%20quotes.pdf
- access: closed
  analysis: ''
  created_date: 2022-03-16
  dependencies: [Gopher, GopherCite Preference dataset]
  description: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality: {}
  model_card: ''
  monitoring: ''
  name: GopherCite reward model
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  size: ''
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://storage.googleapis.com/deepmind-media/Teaching%20language%20models%20to%20support%20answers%20with%20verified%20quotes/Teaching%20language%20models%20to%20support%20answers%20with%20verified%20quotes.pdf
- access: closed
  analysis: ''
  created_date: 2022-03-16
  datasheet: ''
  dependencies: [Gopher, Google Search]
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality: {}
  monitoring: ''
  name: GopherCite Preference dataset
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 33k response pairs
  type: dataset
  url: https://storage.googleapis.com/deepmind-media/Teaching%20language%20models%20to%20support%20answers%20with%20verified%20quotes/Teaching%20language%20models%20to%20support%20answers%20with%20verified%20quotes.pdf
- access: closed
  analysis: ''
  created_date: 2022-09-29
  dependencies: [Chinchilla]
  description: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality:
    explanation: text
    value: text; text
  model_card: ''
  monitoring: ''
  name: Dramatron
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  size: 70B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2209.14958

- access: open
  analysis: Evaluated on evaluation trajectories and SoTA baselines using robotic data.
  created_date: 2023-07-28
  dependencies: [PaLI-X, PaLM-E, RT-2 action tokens]
  description: RT-2 is a vision-language-action model for robotic actions that incorporates chain of thought reasoning.
  feedback: ''
  intended_uses: ''
  license:
    value: unknown
  modality:
    explanation: RT-2 takes in images of its surroundings and natural language prompts and makes the according robotic actions
    value: text, video; other
  model_card: ''
  monitoring: ''
  name: RT-2
  organization: DeepMind
  prohibited_uses: ''
  quality_control: ''
  size: 55B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2307.15818.pdf
