---
- access: open
  analysis: Evaluated on a range of benchmarks and performed on par with LLaMA-7B.
  created_date: 2023-05-05
  dependencies: [RedPajama-Data, C4, The Stack, Multimodal C4]
  description: MPT is a series of large language models seeking to address the limitations
    of other open source models like LLaMA and Pythia.
  feedback: ''
  intended_uses: ''
  license: Apache 2.0
  modality:
    explanation: text
    value: text; text
  model_card: ''
  monitoring: ''
  name: MPT
  organization: Mosaic
  prohibited_uses: ''
  quality_control: ''
  size: 7B parameters (dense)
  training_emissions: unknown
  training_hardware: 440 A100 40GB GPUs
  training_time: 9.5 days
  type: model
  url: https://www.mosaicml.com/blog/mpt-7b
