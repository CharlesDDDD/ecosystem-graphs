---
- access: open
  analysis: Reports results on the Vicuna benchmark and compares performance level
    and time expenditure with ChatGPT
  created_date: 2023-05-23
  dependencies: [QLoRA, OASST1]
  description: Guanaco is a model family trained with QLORA, an efficient finetuning
    approach that reduces memory usage enough to finetune a 65B parameter model
    on a single 48GB GPU while preserving full 16-bit finetuning task performance.
  feedback: ''
  intended_uses: ''
  license: MIT
  modality:
    explanation: natural language text
    value: text; text
  model_card: ''
  monitoring: ''
  name: Guanaco
  organization: University of Washington
  prohibited_uses: ''
  quality_control: ''
  size: 33B parameters (dense)
  training_emissions: ''
  training_hardware: A single 24 GB GPU
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2305.14314v1.pdf
